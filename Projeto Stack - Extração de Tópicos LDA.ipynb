{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mineração de dados com Stack\n",
    "\n",
    "## Extração de tópicos com LDA\n",
    "\n",
    "**Tarefas:**\n",
    "\n",
    "* Extrair tópicos usando LDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pacotes e configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "import operator\n",
    "import os\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from elasticsearch import client\n",
    "\n",
    "from random import random\n",
    "\n",
    "# html render para o notebook\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# https://scikit-learn.org/stable/install.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lendo o arquivo\n",
    "\n",
    "* Os dados utilizados foram pré-processados e indexados em uma base do Elastic.\n",
    "* Para agilizar as consultas e testes, um dataset foi gerado com um sub-conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperando os dados\n",
    "df_dados = pd.read_pickle(\"results_elastic.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20489633</td>\n",
       "      <td>2013-12-10T08:39:25.207</td>\n",
       "      <td>&lt;p&gt;I had the same problem. Installing the root...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3888726</td>\n",
       "      <td>2010-10-08T08:02:58.167</td>\n",
       "      <td>&lt;p&gt;By enabling virtualization from the BIOS se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46169264</td>\n",
       "      <td>2017-09-12T06:21:45.053</td>\n",
       "      <td>&lt;p&gt;See this link. This resolved this issue, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8745880</td>\n",
       "      <td>2012-01-05T16:21:24.913</td>\n",
       "      <td>&lt;p&gt;The version of ImageMagick in the CentOS yu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11134318</td>\n",
       "      <td>2012-06-21T08:29:00.500</td>\n",
       "      <td>&lt;p&gt;Umm, I would say try again. The file stated...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id             CreationDate  \\\n",
       "0  20489633  2013-12-10T08:39:25.207   \n",
       "1   3888726  2010-10-08T08:02:58.167   \n",
       "2  46169264  2017-09-12T06:21:45.053   \n",
       "3   8745880  2012-01-05T16:21:24.913   \n",
       "4  11134318  2012-06-21T08:29:00.500   \n",
       "\n",
       "                                                Body  \n",
       "0  <p>I had the same problem. Installing the root...  \n",
       "1  <p>By enabling virtualization from the BIOS se...  \n",
       "2  <p>See this link. This resolved this issue, th...  \n",
       "3  <p>The version of ImageMagick in the CentOS yu...  \n",
       "4  <p>Umm, I would say try again. The file stated...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alguns registros recuperados.\n",
    "# O conteúdo que será usado é a coluna Body\n",
    "df_dados[[\"Id\",\"CreationDate\", \"Body\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraindo categorias dos Dados\n",
    "\n",
    "Os códigos a seguir foram baseados nos materiais publicados abaixo.\n",
    "\n",
    "* **Material de ajuda**\n",
    " * https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925\n",
    " * https://github.com/priya-dwivedi/Deep-Learning/blob/master/topic_modeling/LDA_Newsgroup.ipynb\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim e NLTK são duas bibliotecas para trabalhar com\n",
    "# NLP\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# O pacode wordnet é necessário para trabalhar com o dicionário\n",
    "# em inglês. Caso não esteja instalado, descomentar a linha abaixo.\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processando os textos que serão utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Stemming é uma redução de palavras para a sua formação raíz.\n",
    "Dessa forma o sistema consegue identificar palavras relacionadas.\n",
    "'''\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def nlp_lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def nlp_preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(nlp_lemmatize_stemming(token))            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo de uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>By enabling virtualization from the BIOS setup of the main machine it worked!\\n<a href=\"https://stackoverflow.com/questions/744211/problem-installing-x64-guest-os-with-vmware-server\">Problem installing x64 guest OS with vmware Server</a></p>\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# O primeiro registro no dataset contém o seguinte texto\n",
    "display(df_dados[\"Body\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enabl', 'virtual', 'bio', 'setup', 'main', 'machin', 'work', 'href', 'https', 'stackoverflow', 'question', 'problem', 'instal', 'guest', 'vmware', 'server', 'problem', 'instal', 'guest', 'vmware', 'server']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fazendo processamento com as fuções de stemming:\n",
    "- O texto original é separado em palavras únicas.\n",
    "- Cada palavra é reduzida (via stemming) à sua base.\n",
    "\"\"\"\n",
    "print(nlp_preprocess(df_dados[\"Body\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processando todo o dataset (resultado da pesquisa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processa todos os documentos (coluna Body)\n",
    "# do dataset\n",
    "nlp_processed_docs = []\n",
    "\n",
    "for item in df_dados[\"Body\"]:\n",
    "    nlp_processed_docs.append(nlp_preprocess(item))\n",
    "    \n",
    "# 300 documentos\n",
    "len(nlp_processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando uma Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Uma vez que todas as palavras dos registros foram processados\n",
    "é preciso criar uma bag of words. A biblioteca gensim será utilizada.\n",
    "'''\n",
    "nlp_dictionary = gensim.corpora.Dictionary(nlp_processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1333"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quantas palavras no dicionário?\n",
    "len(nlp_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Uma vez criado a bag of words, é possível apliar um filtro\n",
    "para remover palavras raras e/ou muito comuns (opcional)\n",
    "\n",
    "- palavras com ocorrência inferior a 15 vezes (raras).\n",
    "- palavras com ocorrência em mais de 10% do total de documentos.\n",
    "'''\n",
    "nlp_dictionary.filter_extremes(no_below=5, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quantas palavras no dicionário filtrado?\n",
    "len(nlp_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Agora é possível criar um modelo de bag of words para os documentos.\n",
    "Pra cada documento será criado um dicionário com a contagem de frequência \n",
    "de cada palavra no documento.\n",
    "'''\n",
    "nlp_bow_corpus = [nlp_dictionary.doc2bow(doc) for doc in nlp_processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavra 27 (\"blockquot\") = 6 ocorrências.\n",
      "Palavra 30 (\"strong\") = 2 ocorrências.\n",
      "Palavra 31 (\"support\") = 1 ocorrências.\n",
      "Palavra 58 (\"django\") = 3 ocorrências.\n",
      "Palavra 59 (\"final\") = 1 ocorrências.\n",
      "Palavra 60 (\"fixtur\") = 6 ocorrências.\n",
      "Palavra 61 (\"initial_data\") = 4 ocorrências.\n",
      "Palavra 62 (\"json\") = 1 ocorrências.\n",
      "Palavra 63 (\"know\") = 2 ocorrências.\n",
      "Palavra 64 (\"loaddata\") = 2 ocorrências.\n",
      "Palavra 65 (\"manag\") = 2 ocorrências.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Uma vez que o modelo é criado, é possível consultar a frequência de palavras \n",
    "em cada documento.\n",
    "'''\n",
    "# 20o documento do registro.\n",
    "nlp_bow_corpus_doc = nlp_bow_corpus[20]\n",
    "\n",
    "for i in range(len(nlp_bow_corpus_doc)):\n",
    "    print(\"Palavra {} (\\\"{}\\\") = {} ocorrências.\".format(nlp_bow_corpus_doc[i][0], \n",
    "                                                         nlp_dictionary[nlp_bow_corpus_doc[i][0]], \n",
    "                                                         nlp_bow_corpus_doc[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executando o LDA\n",
    "\n",
    "* O LDA (latent Dirichlet allocation) é um modelo estatístico que pode ser aplicado a um conjunto de dados.\n",
    "* Pode ser utilizado para descoberta de tópicos em documentos.\n",
    "* No dataset utilizado, ainda não existem os registros do tópicos relacionados a cada um dos documentos.\n",
    "* Nesse caso, o LDA será utilizado para fazer uma descoberta simples de palavras que podem ser consideradas relacionadas e pertencentes a um tópico em particular.\n",
    "\n",
    "\n",
    "* **TODO**:\n",
    "    * Ampliar o dataset utilizado com as identificações das categorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Referência:\n",
    "\n",
    "LDA mono-core -- fallback code in case LdaMulticore throws an error on your machine\n",
    "lda_model = gensim.models.LdaModel(bow_corpus, \n",
    "                                   num_topics = 10, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 50)\n",
    "                                   \n",
    "https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\"\"\"\n",
    "\n",
    "# LDA multicore\n",
    "lda_model =  gensim.models.LdaMulticore(nlp_bow_corpus, \n",
    "                                        num_topics = 8, \n",
    "                                        id2word = nlp_dictionary,                                    \n",
    "                                        passes = 10,\n",
    "                                        workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(60, 0.10404698),\n",
       " (62, 0.06982548),\n",
       " (143, 0.039006505),\n",
       " (65, 0.03753074),\n",
       " (61, 0.036499448),\n",
       " (58, 0.03621276),\n",
       " (161, 0.03180927),\n",
       " (27, 0.03164836),\n",
       " (105, 0.03138134),\n",
       " (18, 0.020724475)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.get_topic_terms(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tópico: 0 \n",
      "Palavras: 0.104*\"fixtur\" + 0.070*\"json\" + 0.039*\"load\" + 0.038*\"manag\" + 0.036*\"initial_data\" + 0.036*\"django\" + 0.032*\"model\" + 0.032*\"blockquot\" + 0.031*\"test\" + 0.021*\"updat\"\n",
      "\n",
      "\n",
      "Tópico: 1 \n",
      "Palavras: 0.067*\"question\" + 0.065*\"eclips\" + 0.053*\"sudo\" + 0.038*\"local\" + 0.031*\"applic\" + 0.029*\"stackoverflow\" + 0.026*\"chang\" + 0.026*\"run\" + 0.026*\"directori\" + 0.020*\"like\"\n",
      "\n",
      "\n",
      "Tópico: 2 \n",
      "Palavras: 0.083*\"visual\" + 0.064*\"studio\" + 0.051*\"compos\" + 0.047*\"creat\" + 0.044*\"requir\" + 0.035*\"project\" + 0.035*\"nuget\" + 0.031*\"microsoft\" + 0.030*\"resolv\" + 0.021*\"download\"\n",
      "\n",
      "\n",
      "Tópico: 3 \n",
      "Palavras: 0.217*\"strong\" + 0.060*\"server\" + 0.038*\"fix\" + 0.035*\"microsoft\" + 0.029*\"data\" + 0.029*\"librari\" + 0.027*\"need\" + 0.026*\"download\" + 0.025*\"tool\" + 0.018*\"support\"\n",
      "\n",
      "\n",
      "Tópico: 4 \n",
      "Palavras: 0.054*\"command\" + 0.050*\"rubi\" + 0.043*\"brew\" + 0.038*\"ubuntu\" + 0.037*\"build\" + 0.028*\"sudo\" + 0.027*\"line\" + 0.026*\"tool\" + 0.025*\"mingw\" + 0.023*\"modul\"\n",
      "\n",
      "\n",
      "Tópico: 5 \n",
      "Palavras: 0.073*\"stack\" + 0.069*\"imgur\" + 0.061*\"updat\" + 0.044*\"imag\" + 0.038*\"extens\" + 0.032*\"enter\" + 0.032*\"descript\" + 0.029*\"have\" + 0.026*\"rstudio\" + 0.025*\"face\"\n",
      "\n",
      "\n",
      "Tópico: 6 \n",
      "Palavras: 0.062*\"android\" + 0.038*\"strong\" + 0.036*\"download\" + 0.034*\"blockquot\" + 0.029*\"java\" + 0.028*\"question\" + 0.024*\"eclips\" + 0.024*\"django\" + 0.023*\"librari\" + 0.022*\"ubuntu\"\n",
      "\n",
      "\n",
      "Tópico: 7 \n",
      "Palavras: 0.082*\"node\" + 0.072*\"github\" + 0.059*\"issu\" + 0.035*\"framework\" + 0.033*\"have\" + 0.032*\"answer\" + 0.025*\"fix\" + 0.021*\"hope\" + 0.021*\"googl\" + 0.021*\"fine\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Uma vez que o modelo do LDA é criado, podemos consultar os resultados:\n",
    "\n",
    "-  Cada tópico encontrado pelo LDA, será exibido:\n",
    "    - palavra * probabilidade da palavra no documento de acordo com o LDA\n",
    "\n",
    "Documentação do gensim:\n",
    "num_topics (int, optional) – The number of topics to be selected, if -1 - all topics will be in result (ordered by significance).\n",
    "num_words (int, optional) – The number of words to be included per topics (ordered by significance).\n",
    "\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\"\"\"\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Tópico: {} \\nPalavras: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fixtur', 0.10404698),\n",
       " ('json', 0.06982548),\n",
       " ('load', 0.039006505),\n",
       " ('manag', 0.03753074),\n",
       " ('initial_data', 0.036499448),\n",
       " ('django', 0.03621276),\n",
       " ('model', 0.03180927),\n",
       " ('blockquot', 0.03164836),\n",
       " ('test', 0.03138134),\n",
       " ('updat', 0.020724475)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Da documentação do gensim:\n",
    "\n",
    "show_topic(topicid, topn=10)\n",
    "\n",
    "Parameters:\n",
    " topicid (int) – The ID of the topic to be returned\n",
    " topn (int, optional) – Number of the most significant words that are associated with the topic.\n",
    "\n",
    "Returns:\n",
    "  Word - probability pairs for the most relevant words generated by the topic.\n",
    "  Return type:\n",
    "       list of (str, float)\n",
    "\n",
    "\"\"\"\n",
    "# Exibindo tópico individual\n",
    "lda_model.show_topic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuação\n",
    "\n",
    "* Os resultados obtidos definem:\n",
    " * Para cada tópico encontrado, temos a lista de palavras (+ pesos ou probabilidades) das palavras mais representativas para o determinado tópico.\n",
    " * Se no dataset original a informação de cada tópico já existir, a partir daqui é possível criar um classificador de dados onde:\n",
    " * Para cada conjunto de palavras (dicionário) de um documento, o LDA identifica qual dos tópicos é o mais adequado para o documento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
